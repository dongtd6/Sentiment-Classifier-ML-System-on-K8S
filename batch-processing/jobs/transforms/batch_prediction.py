# jobs/transforms/batch_prediction.py
import math
import os

import pyspark.sql.functions as F
import requests
import yaml

BASE_DIR = os.path.dirname(os.path.dirname(__file__))  # /job/jobs
CONFIG_PATH = os.path.join(BASE_DIR, "configs", "config.yml")
with open(CONFIG_PATH) as f:
    cfg = yaml.safe_load(f)


def batch_predict(spark, dataframe):
    """
    dataframe: Spark DataFrame cÃ³ Ã­t nháº¥t cá»™t 'review' (string)
    return: DataFrame gá»“m 2 cá»™t [review, sentiment]
    """
    PREDICT_URL = cfg["predict"]["api_url"]
    BATCH_SIZE = cfg["predict"]["batch_size"]

    # Láº¥y list cÃ¡c review cáº§n predict
    rows = (
        dataframe.select("review")
        .where(F.col("review").isNotNull() & (F.length("review") > 0))
        .rdd.map(lambda r: r[0])
        .collect()
    )

    pending_count = len(rows)
    print(f"ğŸ•’ Sá»‘ dÃ²ng cáº§n predict: {pending_count}")
    if pending_count == 0:
        print("âœ… KhÃ´ng cÃ²n dÃ²ng nÃ o cáº§n dá»± Ä‘oÃ¡n. Káº¿t thÃºc.")
        return dataframe.withColumn("sentiment", F.lit(None).cast("string"))

    total_batches = math.ceil(pending_count / BATCH_SIZE)

    def chunks(lst, n):
        for i in range(0, len(lst), n):
            yield lst[i : i + n]

    all_updates = []  # Ä‘á»ƒ collect káº¿t quáº£ toÃ n bá»™

    for idx, chunk in enumerate(chunks(rows, BATCH_SIZE), start=1):
        unique_comments = list(dict.fromkeys(chunk))
        print(f"ğŸ“® LÃ´ {idx}/{total_batches}: gá»­i {len(unique_comments)} cÃ¢u ...")

        try:
            resp = requests.post(
                PREDICT_URL, json={"comments": unique_comments}, timeout=60
            )
            if resp.status_code != 200:
                print(f"âŒ API tráº£ mÃ£ {resp.status_code}: {resp.text[:200]}")
                continue
            body = resp.json()
            preds = body.get("predictions", [])
        except Exception as e:
            print(f"âŒ Lá»—i khi gá»i API: {e}")
            continue

        if not isinstance(preds, list) or len(preds) == 0:
            print("âš ï¸ API khÃ´ng tráº£ vá» 'predictions' há»£p lá»‡.")
            continue

        # Append mapping vÃ o all_updates
        for item in preds:
            cmt = item.get("original_comment")
            sent = item.get("predicted_sentiment")
            if cmt is not None and sent is not None:
                all_updates.append((cmt, sent))

    # Táº¡o DataFrame káº¿t quáº£
    if all_updates:
        df_updates = spark.createDataFrame(all_updates, ["review", "sentiment"])
        print(f"ğŸ“ Tá»•ng cá»™ng {df_updates.count()} dÃ²ng Ä‘Ã£ Ä‘Æ°á»£c predict")
    else:
        # fallback náº¿u khÃ´ng cÃ³ káº¿t quáº£
        df_updates = spark.createDataFrame([], schema="review string, sentiment string")

    return df_updates
