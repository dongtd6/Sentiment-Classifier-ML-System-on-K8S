# extract_job.py
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T
from pyspark.sql.functions import col

if __name__ == "__main__":
    spark = (
        SparkSession.builder.master("local[*]")
        .config("spark.ui.port", "4042")
        .appName("Python Spark SQL basic example")
        .config("spark.jars", "jars/postgresql-42.6.0.jar")
        .getOrCreate()
    )
    # Define the schema for the devices table
    dataframe = (
        spark.read.format("jdbc")
        .option(
            "url", "jdbc:postgresql://localhost:5432/crm_db"
        )
        .option("dbtable", "product_reviews")
        .option("user", "pgadmin")
        .option("password", "pw123")
        .option("driver", "org.postgresql.Driver")
        .load()
    )

    # jdbc:postgresql://localhost:5432/
    # jdbc:postgresql://postgresql.storage.svc.cluster.local:5432/crm_db"

    dataframe.show()

    dataframe.persist()  # cache the DataFrame in memory

    dataframe.printSchema()

    print(f"Read {dataframe.count()} rows from PostgreSQL.")

    # --- Spark SQL Transformations ---
    df_transformed = dataframe.select(
        col("review_id"),
        col("product_id"),
        col("review"),
        col("created_at"),
        col("source"),
    )

    # Show the transformed DataFrame
    df_transformed.show()

    # Write the DataFrame to a Delta table



    spark.stop()

    # while True:
    #     pass

    # # Write the DataFrame to a Delta table
    # delta_table_path = "s3a://tsc-bucket/delta/devices"
    # hive_table_name = "devices_delta_lake"

    # df_transformed.write.format("delta").mode("overwrite").option(
    #     "path", delta_table_path
    # ).saveAsTable(hive_table_name)

    # print(f"Data written to Delta Lake at {delta_table_path} and Hive table {hive_table_name}.")

    # # --- Spark SQL Queries ---

    # # Example of a simple Spark SQL query
    #
    # --- Spark SQL Transformations ---

    #
    # --- Spark SQL Aggregations ---

    # Stop the SparkSession
