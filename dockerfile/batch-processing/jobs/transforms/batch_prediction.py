# jobs/transforms/batch_prediction.py
import math
import os

import pyspark.sql.functions as F
import requests
import yaml

BASE_DIR = os.path.dirname(os.path.dirname(__file__))  # /job/jobs
CONFIG_PATH = os.path.join(BASE_DIR, "configs", "config.yml")
with open(CONFIG_PATH) as f:
    cfg = yaml.safe_load(f)


def batch_predict(spark, dataframe, logger):
    """
    dataframe: Spark DataFrame c√≥ √≠t nh·∫•t c·ªôt 'review' (string)
    return: DataFrame g·ªìm 2 c·ªôt [review, sentiment]
    """
    PREDICT_URL = cfg["predict"]["api_url"]
    BATCH_SIZE = cfg["predict"]["batch_size"]

    # L·∫•y list c√°c review c·∫ßn predict
    rows = (
        dataframe.select("review")
        .where(F.col("review").isNotNull() & (F.length("review") > 0))
        .rdd.map(lambda r: r[0])
        .collect()
    )

    pending_count = len(rows)
    logger.info(f"üïí S·ªë d√≤ng c·∫ßn predict: {pending_count}")
    if pending_count == 0:
        logger.info("‚úÖ Kh√¥ng c√≤n d√≤ng n√†o c·∫ßn d·ª± ƒëo√°n. K·∫øt th√∫c.")
        return dataframe.withColumn("sentiment", F.lit(None).cast("string"))

    total_batches = math.ceil(pending_count / BATCH_SIZE)

    def chunks(lst, n):
        for i in range(0, len(lst), n):
            yield lst[i : i + n]

    all_updates = []  # ƒë·ªÉ collect k·∫øt qu·∫£ to√†n b·ªô

    for idx, chunk in enumerate(chunks(rows, BATCH_SIZE), start=1):
        unique_comments = list(dict.fromkeys(chunk))
        logger.info(f"üìÆ L√¥ {idx}/{total_batches}: g·ª≠i {len(unique_comments)} c√¢u ...")

        try:
            resp = requests.post(
                PREDICT_URL, json={"comments": unique_comments}, timeout=60
            )
            if resp.status_code != 200:
                logger.info(f"‚ùå API tr·∫£ m√£ {resp.status_code}: {resp.text[:200]}")
                continue
            body = resp.json()
            preds = body.get("predictions", [])
        except Exception as e:
            logger.info(f"‚ùå L·ªói khi g·ªçi API: {e}")
            continue

        if not isinstance(preds, list) or len(preds) == 0:
            logger.info("‚ö†Ô∏è API kh√¥ng tr·∫£ v·ªÅ 'predictions' h·ª£p l·ªá.")
            continue

        # Append mapping v√†o all_updates
        for item in preds:
            cmt = item.get("original_comment")
            sent = item.get("predicted_sentiment")
            if cmt is not None and sent is not None:
                all_updates.append((cmt, sent))

    # T·∫°o DataFrame k·∫øt qu·∫£
    if all_updates:
        df_updates = spark.createDataFrame(all_updates, ["review", "sentiment"])
        logger.info(f"üìù T·ªïng c·ªông {df_updates.count()} d√≤ng ƒë√£ ƒë∆∞·ª£c predict")
    else:
        # fallback n·∫øu kh√¥ng c√≥ k·∫øt qu·∫£
        df_updates = spark.createDataFrame([], schema="review string, sentiment string")

    return df_updates
